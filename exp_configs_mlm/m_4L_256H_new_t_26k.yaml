# Author: Roman Jan√≠k
# Experiment YAML configuration file
#
# Notes:
#  Paths are relative to training script working directory.
#  Datasets are preprocessed to Hugging Face arrow format

desc: "Mini model, 4L 256H. New tokenizer 26k vocab. Basic setting, 100 epochs on combined Pero OCR Books and Pero OCR Periodicals."

tokenizer: "../resources/tokenizers/new_tokenizer_26k"

models:
  trained_model:
    name: "Czech RoBERTa small PyTorch 4L_256H"
    desc: "Czech version of own small RoBERTa transformer model. 4 layers, 256 hidden size. Hugging Face PyTorch type."
    config: "exp_configs_models/m_4L_256H_new_t_26k.json"
  old_model:
    name: "RobeCzech base PyTorch"
    desc: "Czech version of RoBERTa transformer model. Hugging Face PyTorch type."
    path: "../resources/robeczech-base-pytorch"

datasets:
  new_tokenizer_dts:
    name: "Pero OCR prepared dataset - new tokenizer"
    desc: "Pero OCR prepared dataset. Concatenated Pero OCR Books and Pero OCR Periodicals datasets.
     Dataset is tokenized, then concatenated and chunked by model input size. Dataset is ready for training.."
    path: "../../datasets/pero_ocr_prepared/new_tokenizer_dts"
  new_tokenizer_26k_dts:
    name: "Pero OCR prepared dataset - new tokenizer with 26k vocabulary"
    desc: "Pero OCR prepared dataset. Concatenated Pero OCR Books and Pero OCR Periodicals datasets.
       Dataset is tokenized, then concatenated and chunked by model input size. Dataset is ready for training.."
    path: "../../datasets/pero_ocr_prepared/new_tokenizer_26k_dts"
  old_tokenizer_dts:
    name: "Pero OCR prepared dataset - old tokenizer"
    desc: "Pero OCR prepared dataset. Concatenated Pero OCR Books and Pero OCR Periodicals datasets.
     Dataset is tokenized, then concatenated and chunked by model input size. Dataset is ready for training.."
    path: "../../datasets/pero_ocr_prepared/old_tokenizer_dts"

training:
  num_train_epochs: 100
  batch_size: 128

  optimizer:
    learning_rate: 1.e-3
    weight_decay: 0.1
    beta1: 0.9
    beta2: 0.98
  lr_scheduler:
    name: "cosine"
    warmup_ratio: 0.04
