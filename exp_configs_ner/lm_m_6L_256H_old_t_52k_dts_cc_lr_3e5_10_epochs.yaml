# Author: Roman Jan√≠k
# Experiment YAML configuration file
#
# Notes:
#  Paths are relative to training script working directory.
#  Datasets are preprocessed to Hugging Face arrow format

datasets:
  chnec:
    desc: Czech Historical Named Entity Corpus 1.0 dataset. Historic-language Czech
      NER dataset.
    name: CHNEC 1.0
    path: ../../datasets/chnec1.0
  cnec:
    desc: Czech Named Entity Corpus 2.0 CoNNL dataset. General-language Czech NER
      dataset.
    name: CNEC 2.0 CoNLL
    path: ../../datasets/cnec2.0_extended
desc: NER Pero OCR LM model. Learning rate set to 3e-5. Learning rate scheduler is
  linear, training 10 epochs on combined CNEC and CHNEC.
model:
  desc: Czech version of own small RoBERTa transformer model. 6 layers, 256 hidden
    size. Hugging Face PyTorch type.
  name: Czech RoBERTa small PyTorch 6L_256H
  path: ../resources/mlm_models/m_6L_256H_old_t_52k
training:
  batch_size: 32
  lr_scheduler:
    name: linear
    warmup_ratio: 0.0
  num_train_epochs: 10
  optimizer:
    beta1: 0.9
    beta2: 0.999
    learning_rate: 3.0e-05
    weight_decay: 0.0
  val_batch_size: 32
