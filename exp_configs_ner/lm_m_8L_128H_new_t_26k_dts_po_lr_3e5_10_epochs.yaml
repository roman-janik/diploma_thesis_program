# Author: Roman Jan√≠k
# Experiment YAML configuration file
#
# Notes:
#  Paths are relative to training script working directory.
#  Datasets are preprocessed to Hugging Face arrow format

datasets:
  poner:
    desc: Pero OCR NER 1.0 dataset. Historic-language Czech OCR-sourced NER dataset.
    name: PONER 1.0
    path: ../../datasets/poner1.0
desc: NER Pero OCR LM model. Learning rate set to 3e-5. Learning rate scheduler is
  linear, training 10 epochs on PONER.
model:
  desc: Czech version of own small RoBERTa transformer model. 8 layers, 128 hidden
    size. Hugging Face PyTorch type.
  name: Czech RoBERTa small PyTorch 8L_128H
  path: ../resources/ml_models/m_8L_128H_new_t_26k
training:
  batch_size: 32
  lr_scheduler:
    name: linear
    warmup_ratio: 0.0
  num_train_epochs: 10
  optimizer:
    beta1: 0.9
    beta2: 0.999
    learning_rate: 3.0e-05
    weight_decay: 0.0
  val_batch_size: 32
